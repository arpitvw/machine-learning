{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   col1  col2\n",
      "0     0     0\n",
      "1     1     1\n",
      "2     4     2\n",
      "3     9     3\n",
      "4    16     4\n",
      "5    25     5\n",
      "6    36     6\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import time\n",
    "d = {'col1': [0,1,4,9,16,25,36], 'col2': [0,1,2,3,4,5,6]}\n",
    "df = pd.DataFrame(data=d)\n",
    "print (df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (7, 2)\n",
      "7\n",
      "Training data size:  (1, 7)\n",
      "a4 staring: [[ 0.99248206  0.99249104  0.99249518  0.99249567  0.99249578  0.99249581\n",
      "   0.99249581]\n",
      " [ 0.99864512  0.9986479   0.99864919  0.99864935  0.99864938  0.99864939\n",
      "   0.99864939]\n",
      " [ 0.99000425  0.99001728  0.99002325  0.99002396  0.99002411  0.99002415\n",
      "   0.99002416]\n",
      " [ 0.98590924  0.98592842  0.9859375   0.98593862  0.98593885  0.98593891\n",
      "   0.98593893]\n",
      " [ 0.99534357  0.99535189  0.99535575  0.99535621  0.99535631  0.99535634\n",
      "   0.99535634]\n",
      " [ 0.99867966  0.99868204  0.99868312  0.99868325  0.99868328  0.99868329\n",
      "   0.99868329]\n",
      " [ 0.99210608  0.99211779  0.99212331  0.99212399  0.99212413  0.99212417\n",
      "   0.99212418]]\n",
      "loss: (7, 7)\n",
      "a4: (7, 7)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n## back propagation\\n#print (a4.shape)\\n#print (w4.shape)\\n    d\\n    dz4=-np.exp(-z4)\\n    dw4=np.dot(dz4,np.transpose(a3))\\n    w4=w4-(.01*(dw4/X_training_data.shape[1]))\\n    #print (np.around(np.log(a4),2))\\n    if np.around(np.log(a4),2).all== 0.00:\\n        print (\"counter:\",i)\\n        break\\n    dz3=a3-a4*np.linalg.svd(w4)\\n    print (dz3)\\n    dw3=np.dot(dz3,np.transpose(a2))\\n    w3=w3-(.1*(dw3/X_training_data.shape[1]))\\n    dz2=-np.exp(-z2)\\n    dw2=np.dot(dz2,np.transpose(a1))\\n    w2=w2-(.1*(dw2/X_training_data.shape[1]))\\n    dz1=-np.exp(-z1)\\n    dw1=np.dot(dz1,np.transpose(X_training_data))\\n    w1=w1-(.1*(dw1/X_training_data.shape[1]))\\n    z1=np.dot(w1,X_training_data)\\n    a1=sigmoid(z1)\\n    z2=np.dot(w2,a1)\\n    a2=sigmoid(z2)\\n    z3=np.dot(w3,a2)\\n    a3=sigmoid(z3)\\n    z4=np.dot(w4,a3)\\n    a4=sigmoid(z4)\\n    ax.plot(z4,loss,\\'*\\')\\n    fig.canvas.draw()\\n    \\n    \\n    \\n\\n    \\nloss=(np.log(a4))\\nprint (\"loss:\",loss)\\noutput=np.sum(softmax(a4),axis=0)\\nprint (\"a4 after:\",a4)\\n#print (\"output:\",a4)\\n#print (\"sum:\",output)\\n\\n\\nprint (dw1)\\nprint (w1)\\nprint (z4.shape)\\nprint (dw4.shape)\\n#print (Features)\\n#dw4=\\n#dw4=\\n#print (a4.shape)\\n#for i in w4:\\n#    dw4=a4-1\\n\\nx_test=np.array([])'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d=1\n",
    "\n",
    "learning_rate=0.1\n",
    "#fig = plt.figure(figsize=(5, 5))\n",
    "#ax = fig.add_subplot(111)\n",
    "#plt.ion()\n",
    "#fig.show()\n",
    "#fig.canvas.draw()\n",
    "df.describe\n",
    "X_input_data=df.values\n",
    "X_training_data=np.transpose(X_input_data[:,:-1]) #Please check labels should be in last column\n",
    "\n",
    "print (\"shape:\",X_input_data.shape)\n",
    "Y_label=np.transpose(X_input_data[:,-1])\n",
    "training_count=X_training_data.shape[1]\n",
    "#features=X_training_data.shape[0]\n",
    "print (training_count)\n",
    "\n",
    "\n",
    "print ('Training data size: ',X_training_data.shape) \n",
    "#print ('Describe: ',df.describe())\n",
    "Features=X_training_data.shape[0]\n",
    "#dict_layers={1:15,2:12,3:10,4:7}\n",
    "\n",
    "def sigmoid(x,driv=False):\n",
    "    if driv:\n",
    "        return x*(1-x)\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def softmax(x):\n",
    "    t=np.exp(x)\n",
    "    sum1=np.sum(t,axis=0)\n",
    "    return t/sum1\n",
    "def weights(neuron,input_features):\n",
    "    return (np.random.uniform(1,0,size=(neuron,input_features)))\n",
    "def hypothesis(weights,input_features):\n",
    "    return np.dot(weights,input_features)\n",
    "actualop=np.identity(7)\n",
    "\n",
    "w1=weights(15,Features)\n",
    "z1=hypothesis(w1,X_training_data)\n",
    "a1=sigmoid(z1)\n",
    "\n",
    "w2=weights(12,15)\n",
    "z2=hypothesis(w2,a1)\n",
    "a2=sigmoid(z2)\n",
    "\n",
    "w3=weights(10,12)\n",
    "z3=hypothesis(w3,a2)\n",
    "a3=sigmoid(z3)\n",
    "\n",
    "w4=weights(7,10)\n",
    "z4=hypothesis(w4,a3)\n",
    "a4=sigmoid(z4)\n",
    "print (\"a4 staring:\",a4)\n",
    "\n",
    "\n",
    "#print (\"a4:\",a4)\n",
    "output=softmax(a4)\n",
    "#for i in range(1000):\n",
    "loss=np.dot(actualop,np.log(a4))#need to chnage as need to take log accroding to actualop\n",
    "#a=np.log([a4[j,k]  for j in range(7) for k in range(7) if Y_label[j]==k])\n",
    "print (\"loss:\",loss.shape)\n",
    "print (\"a4:\",a4.shape)\n",
    "da4=loss/a4\n",
    "dz4=a4*(1-a4)\n",
    "#w4=w4-learning_rate*(np.dot(dz4,np.transpose(a3)))\n",
    "\n",
    "da3=np.dot(np.transpose(w4),(a4*(1-a4)))\n",
    "dz3=a3*(1-a3)\n",
    "w3=w3-learning_rate*(np.dot(dz3,np.transpose(a2)))\n",
    "\n",
    "da2=np.dot(np.transpose(w3),(a3*(1-a3)))\n",
    "dz2=a2*(1-a2)\n",
    "w2=w2-learning_rate*(np.dot(dz2,np.transpose(a1)))\n",
    "\n",
    "\n",
    "da1=np.dot(np.transpose(w2),(a2*(1-a2)))\n",
    "dz1=a1*(1-a1)\n",
    "w1=w1-learning_rate*(np.dot(dz1,np.transpose(X_training_data)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_weights=X_training_data.shape[0]*dict_layers[1]+dict_layers[1]*dict_layers[2]+dict_layers[3]*dict_layers[4]\n",
    "weights=np.random.uniform(1,0,size=(total_weights,1))\n",
    "A=[]\n",
    "Z=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "265"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    z.append(weights[dict_layers[1]*X_training_data.shape[0]].reshape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = MLPClassifier(hidden_layer_sizes=(15,12,10,7), max_iter=500, alpha=0.0001,\n",
    "                     solver='sgd', verbose=10,  random_state=21,tol=0.000000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
